{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# SparkSQL\n",
    "Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations.\n",
    "\n",
    "![Sparkcomponents](../images/Sparkcomponents.jpg)\n",
    "\n",
    "The entry point into all functionality in Spark is the SparkSession class through the SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Cargamos la librería externa que nos permite abrir formatos .avro\n",
    "# Para ello debemos descargarnos la librería decom.databricks_spark-avro\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /home/afernandez/.ivy2/jars/com.databricks_spark-avro_2.11-4.0.0.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName('MiPrimeraSparkApp').setMaster('local[*]') #Creamos la configuración\n",
    "sc = pyspark.SparkContext(conf = conf) #Abrimos el contexto de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## 0 · Open SparkSQL Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "\n",
    "sparkSession = pyspark.sql.SparkSession(sc, jsparkSession=None)\n",
    "\n",
    "spark = sparkSession\\\n",
    ".builder\\\n",
    ".master(\"local\")\\\n",
    ".appName(\"holi\")\\\n",
    ".config(\"spark.some.config.option\", \"some-value\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## 1 · Crear Dataframes\n",
    "A **Dataset** is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). The Dataset API is available in Scala and Java.  \n",
    "\n",
    "A **DataFrame** is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.  \n",
    "They can be constructed from a wide array of sources such as,\n",
    "* Structured data files\n",
    "* Tables in Hive\n",
    "* External databases\n",
    "* Existing RDDs\n",
    "  \n",
    "DataFrame is represented by a **Dataset of Rows**. In Scala it's a type alias of Dataset \\[ Row \\]. In Java, Dataset < Row > *(Java)*\n",
    "\n",
    "### Structured Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distdata = sc.parallelize([(1,\"DataScientists\"),(2,\"Developers\"),(3,\"Scrums\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If **schema** is ***None*** SparkSQL infers the schema from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| _1|            _2|\n",
      "+---+--------------+\n",
      "|  1|DataScientists|\n",
      "|  2|    Developers|\n",
      "|  3|        Scrums|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DataFrameData = spark.createDataFrame(Distdata)\n",
    "DataFrameData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* If **schema** is a ***list of column names***, the __type of each column__ will be inferred from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|Importancia|         Curro|\n",
      "+-----------+--------------+\n",
      "|          1|DataScientists|\n",
      "|          2|    Developers|\n",
      "|          3|        Scrums|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DataFrameData = spark.createDataFrame(Distdata,[\"Importancia\",\"Curro\"])\n",
    "DataFrameData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* If **schema** is ***StructType***, which consists of ***a list of StructField*** with ***pyspark.sql.types objects***, those data types *must agree* with original data,else Python will throw an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|Importancia|         Curro|\n",
      "+-----------+--------------+\n",
      "|          1|DataScientists|\n",
      "|          2|    Developers|\n",
      "|          3|        Scrums|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"Importancia\", IntegerType(), True),\n",
    "    StructField(\"Curro\", StringType(), True)])\n",
    "DataFrameData = spark.createDataFrame(Distdata,schema)\n",
    "DataFrameData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Comma Separated Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clientes = spark.read.load(\"../data/clientes.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"false\")\n",
    "clientes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaClientes = (StructType([\n",
    "    StructField(\"DNI\", IntegerType(), True),\n",
    "    StructField(\"Nombre\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientsRDD = clientes.rdd\n",
    "type(clientsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|     DNI|              Nombre|             Address|\n",
      "+--------+--------------------+--------------------+\n",
      "|80000000|Antonio Lopez Ram...|Calle Cantalapied...|\n",
      "|70000000|Francisco Arias S...|Avenida de Americ...|\n",
      "|50000000|Norberto Marias Q...|     Calle Uganda 88|\n",
      "|10000000|Julio Cortazar Ca...|   Calle Bruselas 14|\n",
      "|20000000| Arturo Belano Yañez|Travesia de Calvo...|\n",
      "+--------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clientesNew = spark.createDataFrame(clientsRDD,schemaClientes)\n",
    "clientesNew.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Hadoop formats\n",
    "#### parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(registration_dttm=datetime.datetime(2016, 2, 3, 8, 55, 29), id=1, first_name='Amanda', last_name='Jordan', email='ajordan0@com.com', gender='Female', ip_address='1.197.201.2', cc='6759521864920116', country='Indonesia', birthdate='3/8/1971', salary=49756.53, title='Internal Auditor', comments='1E+02')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usuarios = spark.read.parquet(\"../data/userdata1.parquet\")\n",
    "usuarios.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(registration_dttm=datetime.datetime(2016, 2, 3, 8, 55, 29), id=1, first_name='Amanda', last_name='Jordan', email='ajordan0@com.com', gender='Female', ip_address='1.197.201.2', cc='6759521864920116', country='Indonesia', birthdate='3/8/1971', salary=49756.53, title='Internal Auditor', comments='1E+02')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usuarios2 = spark.read.format(\"parquet\").load(\"../data/userdata1.parquet\")\n",
    "usuarios2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- registration_dttm: timestamp (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- cc: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- birthdate: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usuarios.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "#### Avro\n",
    "In order to read this data format files, the appropiate .jar has to be previously loaded in the SparkContext. (See first command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+\n",
      "|  username|               tweet| timestamp|\n",
      "+----------+--------------------+----------+\n",
      "|    miguno|Rock: Nerf paper,...|1366150681|\n",
      "|BlizzardCS|Works as intended...|1366154481|\n",
      "+----------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter = spark.read.format(\"com.databricks.spark.avro\").load(\"../data/twitter.avro\")\n",
    "twitter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+\n",
      "|  username|               tweet| timestamp|\n",
      "+----------+--------------------+----------+\n",
      "|    miguno|Rock: Nerf paper,...|1366150681|\n",
      "|BlizzardCS|Works as intended...|1366154481|\n",
      "+----------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter2 = spark.read.load(\"../data/twitter.avro\",\n",
    "                     format=\"com.databricks.spark.avro\")\n",
    "twitter2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- username: string (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### pandas Import/Export\n",
    "\n",
    "#### Ensure PyArrow installed\n",
    "If you install PySpark using pip, then PyArrow can be brought in as an extra dependency of the SQL module with the command pip install pyspark[sql]. Otherwise, you must ensure that PyArrow is installed and available on all cluster nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.632424</td>\n",
       "      <td>0.128197</td>\n",
       "      <td>0.580186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.990642</td>\n",
       "      <td>0.133975</td>\n",
       "      <td>0.887129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.440693</td>\n",
       "      <td>0.989978</td>\n",
       "      <td>0.521538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.334727</td>\n",
       "      <td>0.602970</td>\n",
       "      <td>0.374078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.095836</td>\n",
       "      <td>0.347972</td>\n",
       "      <td>0.494767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.897462</td>\n",
       "      <td>0.237528</td>\n",
       "      <td>0.354323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.930376</td>\n",
       "      <td>0.445549</td>\n",
       "      <td>0.511088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.165308</td>\n",
       "      <td>0.253127</td>\n",
       "      <td>0.077179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.017232</td>\n",
       "      <td>0.651973</td>\n",
       "      <td>0.060083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.558802</td>\n",
       "      <td>0.316892</td>\n",
       "      <td>0.671885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.272009</td>\n",
       "      <td>0.615658</td>\n",
       "      <td>0.522949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.393860</td>\n",
       "      <td>0.788612</td>\n",
       "      <td>0.856378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.391185</td>\n",
       "      <td>0.480339</td>\n",
       "      <td>0.844784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.201080</td>\n",
       "      <td>0.120365</td>\n",
       "      <td>0.795468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.450944</td>\n",
       "      <td>0.490562</td>\n",
       "      <td>0.978521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.609231</td>\n",
       "      <td>0.199992</td>\n",
       "      <td>0.879049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.149525</td>\n",
       "      <td>0.292851</td>\n",
       "      <td>0.440360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.487931</td>\n",
       "      <td>0.314952</td>\n",
       "      <td>0.703867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.276619</td>\n",
       "      <td>0.961712</td>\n",
       "      <td>0.307587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.439424</td>\n",
       "      <td>0.619524</td>\n",
       "      <td>0.771399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.347132</td>\n",
       "      <td>0.521692</td>\n",
       "      <td>0.080260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.405689</td>\n",
       "      <td>0.981248</td>\n",
       "      <td>0.322579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.084526</td>\n",
       "      <td>0.099088</td>\n",
       "      <td>0.107087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.163731</td>\n",
       "      <td>0.382261</td>\n",
       "      <td>0.878020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.980823</td>\n",
       "      <td>0.838869</td>\n",
       "      <td>0.694404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.639849</td>\n",
       "      <td>0.133159</td>\n",
       "      <td>0.215354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.115634</td>\n",
       "      <td>0.680656</td>\n",
       "      <td>0.333994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.669431</td>\n",
       "      <td>0.953339</td>\n",
       "      <td>0.037133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.174272</td>\n",
       "      <td>0.483384</td>\n",
       "      <td>0.434209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.399396</td>\n",
       "      <td>0.552330</td>\n",
       "      <td>0.878839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.930476</td>\n",
       "      <td>0.004419</td>\n",
       "      <td>0.667206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.033958</td>\n",
       "      <td>0.838308</td>\n",
       "      <td>0.750971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.023864</td>\n",
       "      <td>0.481619</td>\n",
       "      <td>0.743859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.711756</td>\n",
       "      <td>0.073074</td>\n",
       "      <td>0.572150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.333532</td>\n",
       "      <td>0.164543</td>\n",
       "      <td>0.685617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.695120</td>\n",
       "      <td>0.170069</td>\n",
       "      <td>0.740490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.972681</td>\n",
       "      <td>0.674250</td>\n",
       "      <td>0.456298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.030356</td>\n",
       "      <td>0.024898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.837648</td>\n",
       "      <td>0.055557</td>\n",
       "      <td>0.294045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.626002</td>\n",
       "      <td>0.059792</td>\n",
       "      <td>0.032272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.401532</td>\n",
       "      <td>0.067193</td>\n",
       "      <td>0.123643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.733553</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.837383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.918899</td>\n",
       "      <td>0.772943</td>\n",
       "      <td>0.826287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.468599</td>\n",
       "      <td>0.989319</td>\n",
       "      <td>0.237489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.728980</td>\n",
       "      <td>0.484711</td>\n",
       "      <td>0.595202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.999694</td>\n",
       "      <td>0.606067</td>\n",
       "      <td>0.269453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.067935</td>\n",
       "      <td>0.907574</td>\n",
       "      <td>0.494554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.111118</td>\n",
       "      <td>0.294797</td>\n",
       "      <td>0.110060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.491690</td>\n",
       "      <td>0.985239</td>\n",
       "      <td>0.315698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.376462</td>\n",
       "      <td>0.934219</td>\n",
       "      <td>0.158701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.588166</td>\n",
       "      <td>0.389670</td>\n",
       "      <td>0.817869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.549994</td>\n",
       "      <td>0.125684</td>\n",
       "      <td>0.414754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.306530</td>\n",
       "      <td>0.732551</td>\n",
       "      <td>0.112860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.467651</td>\n",
       "      <td>0.286596</td>\n",
       "      <td>0.679722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.034329</td>\n",
       "      <td>0.628144</td>\n",
       "      <td>0.834254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.466660</td>\n",
       "      <td>0.983677</td>\n",
       "      <td>0.068806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.710655</td>\n",
       "      <td>0.084242</td>\n",
       "      <td>0.630522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.128752</td>\n",
       "      <td>0.163690</td>\n",
       "      <td>0.587127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.132752</td>\n",
       "      <td>0.073241</td>\n",
       "      <td>0.781288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.087168</td>\n",
       "      <td>0.429369</td>\n",
       "      <td>0.585564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2\n",
       "0   0.632424  0.128197  0.580186\n",
       "1   0.990642  0.133975  0.887129\n",
       "2   0.440693  0.989978  0.521538\n",
       "3   0.334727  0.602970  0.374078\n",
       "4   0.095836  0.347972  0.494767\n",
       "5   0.897462  0.237528  0.354323\n",
       "6   0.930376  0.445549  0.511088\n",
       "7   0.165308  0.253127  0.077179\n",
       "8   0.017232  0.651973  0.060083\n",
       "9   0.558802  0.316892  0.671885\n",
       "10  0.272009  0.615658  0.522949\n",
       "11  0.393860  0.788612  0.856378\n",
       "12  0.391185  0.480339  0.844784\n",
       "13  0.201080  0.120365  0.795468\n",
       "14  0.450944  0.490562  0.978521\n",
       "15  0.609231  0.199992  0.879049\n",
       "16  0.149525  0.292851  0.440360\n",
       "17  0.487931  0.314952  0.703867\n",
       "18  0.276619  0.961712  0.307587\n",
       "19  0.439424  0.619524  0.771399\n",
       "20  0.347132  0.521692  0.080260\n",
       "21  0.405689  0.981248  0.322579\n",
       "22  0.084526  0.099088  0.107087\n",
       "23  0.163731  0.382261  0.878020\n",
       "24  0.980823  0.838869  0.694404\n",
       "25  0.639849  0.133159  0.215354\n",
       "26  0.115634  0.680656  0.333994\n",
       "27  0.669431  0.953339  0.037133\n",
       "28  0.174272  0.483384  0.434209\n",
       "29  0.399396  0.552330  0.878839\n",
       "..       ...       ...       ...\n",
       "70  0.930476  0.004419  0.667206\n",
       "71  0.033958  0.838308  0.750971\n",
       "72  0.023864  0.481619  0.743859\n",
       "73  0.711756  0.073074  0.572150\n",
       "74  0.333532  0.164543  0.685617\n",
       "75  0.695120  0.170069  0.740490\n",
       "76  0.972681  0.674250  0.456298\n",
       "77  0.003006  0.030356  0.024898\n",
       "78  0.837648  0.055557  0.294045\n",
       "79  0.626002  0.059792  0.032272\n",
       "80  0.401532  0.067193  0.123643\n",
       "81  0.733553  0.434900  0.837383\n",
       "82  0.918899  0.772943  0.826287\n",
       "83  0.468599  0.989319  0.237489\n",
       "84  0.728980  0.484711  0.595202\n",
       "85  0.999694  0.606067  0.269453\n",
       "86  0.067935  0.907574  0.494554\n",
       "87  0.111118  0.294797  0.110060\n",
       "88  0.491690  0.985239  0.315698\n",
       "89  0.376462  0.934219  0.158701\n",
       "90  0.588166  0.389670  0.817869\n",
       "91  0.549994  0.125684  0.414754\n",
       "92  0.306530  0.732551  0.112860\n",
       "93  0.467651  0.286596  0.679722\n",
       "94  0.034329  0.628144  0.834254\n",
       "95  0.466660  0.983677  0.068806\n",
       "96  0.710655  0.084242  0.630522\n",
       "97  0.128752  0.163690  0.587127\n",
       "98  0.132752  0.073241  0.781288\n",
       "99  0.087168  0.429369  0.585564\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Capacitar la transferencia de datos columnares basados en Arrow\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "    \n",
    "# Generar un DataFrame de pandas\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Crear un DataFrame de Spark a partir de Pandas a través de Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Convertir el DataFrame de Spark de vuelta a Pandas a través de Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()\n",
    "\n",
    "result_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## 2 · DataFrames Operations\n",
    "\n",
    "We can access data in a column by attribute **(df.age)** and by index **(df['age'])**. Better the last way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"null\",\"Michael\"),(30,\"Andy\"),(19,\"Justin\"),(19,\"Alejandra\"),(19,\"Justin\")])\n",
    "df = spark.createDataFrame(rdd,[\"age\",\"name\"])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "|  19|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|     name|(age + 1)|\n",
      "+---------+---------+\n",
      "|  Michael|     null|\n",
      "|     Andy|     31.0|\n",
      "|   Justin|     20.0|\n",
      "|Alejandra|     20.0|\n",
      "|   Justin|     20.0|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['name'], df['age'] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "+----+---------+\n",
      "| age|     name|\n",
      "+----+---------+\n",
      "|  30|     Andy|\n",
      "|  19|Alejandra|\n",
      "|  19|   Justin|\n",
      "|null|  Michael|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.distinct().count())\n",
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  30|    1|\n",
      "|  19|    3|\n",
      "|null|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|     name|(age + 10)|\n",
      "+---------+----------+\n",
      "|  Michael|      null|\n",
      "|     Andy|      40.0|\n",
      "|   Justin|      29.0|\n",
      "|Alejandra|      29.0|\n",
      "|   Justin|      29.0|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"name\"], (df.age + 10)).show()\n",
    "# La función DataFrame.select(*cols)\n",
    "# *cols = lista de nombres de columna o expresiones (Columna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "There is a library plenty of **functions** designed for DataFrames formats, arithmetics... , called **pyspark.sql.functions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|media|\n",
      "+-----+\n",
      "|21.75|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as funciones\n",
    "df.select(funciones.avg(\"age\")\n",
    "          .alias(\"media\")\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|     name|new_age|\n",
      "+---------+-------+\n",
      "|  Michael|   null|\n",
      "|     Andy|  30.74|\n",
      "|   Justin|  19.74|\n",
      "|Alejandra|  19.74|\n",
      "|   Justin|  19.74|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select(df[\"name\"], (df.age + 0.74).alias(\"new_age\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|approximate_age|\n",
      "+---------------+\n",
      "|           null|\n",
      "|             31|\n",
      "|             20|\n",
      "|             20|\n",
      "|             20|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as funciones\n",
    "df2.select(funciones.ceil(\"new_age\").alias(\"approximate_age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## 3 · Running SQL Queries Programmatically\n",
    "To run SQL queries in a DataFrame, we have to create a **temporal view** in the SparkSQL session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|     name|new_age|\n",
      "+---------+-------+\n",
      "|  Michael|   null|\n",
      "|     Andy|  30.74|\n",
      "|   Justin|  19.74|\n",
      "|Alejandra|  19.74|\n",
      "|   Justin|  19.74|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     name|\n",
      "+---------+\n",
      "|Alejandra|\n",
      "|   Justin|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql2DF = spark.sql(\"SELECT name FROM people WHERE new_age<20\")\n",
    "sql2DF.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Running SQL Queries In Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----------+---------+----------------+------+--------------+----------------+---------+---------+---------+----------------+--------+\n",
      "|  registration_dttm| id|first_name|last_name|           email|gender|    ip_address|              cc|  country|birthdate|   salary|           title|comments|\n",
      "+-------------------+---+----------+---------+----------------+------+--------------+----------------+---------+---------+---------+----------------+--------+\n",
      "|2016-02-03 08:55:29|  1|    Amanda|   Jordan|ajordan0@com.com|Female|   1.197.201.2|6759521864920116|Indonesia| 3/8/1971| 49756.53|Internal Auditor|   1E+02|\n",
      "|2016-02-03 18:04:03|  2|    Albert|  Freeman| afreeman1@is.gd|  Male|218.111.175.34|                |   Canada|1/16/1968|150280.17|   Accountant IV|        |\n",
      "+-------------------+---+----------+---------+----------------+------+--------------+----------------+---------+---------+---------+----------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SQLparquetQuery = spark.sql(\"SELECT * FROM parquet.`../data/userdata1.parquet`\")\n",
    "SQLparquetQuery.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## 4 · Global Temporary Views\n",
    "Temporary views in Spark SQL are session-scoped and will disappear if the session that creates it terminates. If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, you can create a global temporary view. **Global temporary view** is tied to a system preserved database **global_temp**, and we must use the qualified name to refer it, e.g. SELECT * FROM global_temp.view1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createGlobalTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "| age|     name|\n",
      "+----+---------+\n",
      "|null|  Michael|\n",
      "|  30|     Andy|\n",
      "|  19|   Justin|\n",
      "|  19|Alejandra|\n",
      "|  19|   Justin|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "| age|     name|\n",
      "+----+---------+\n",
      "|null|  Michael|\n",
      "|  30|     Andy|\n",
      "|  19|   Justin|\n",
      "|  19|Alejandra|\n",
      "|  19|   Justin|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
