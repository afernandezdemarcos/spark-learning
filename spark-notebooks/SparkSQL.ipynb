{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# SparkSQL\n",
    "Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations.\n",
    "\n",
    "![Sparkcomponents](../images/Sparkcomponents.jpg)\n",
    "\n",
    "The entry point into all functionality in Spark is the SparkSession class through the SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Cargamos la librería externa que nos permite abrir formatos .avro\n",
    "# Para ello debemos descargarnos la librería decom.databricks_spark-avro\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /home/afernandez/.ivy2/jars/com.databricks_spark-avro_2.11-4.0.0.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName('MiPrimeraSparkApp').setMaster('local[*]') #Creamos la configuración\n",
    "sc = pyspark.SparkContext(conf = conf) #Abrimos el contexto de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## 0 · Open SparkSQL Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "\n",
    "sparkSession = pyspark.sql.SparkSession(sc, jsparkSession=None)\n",
    "\n",
    "spark = sparkSession\\\n",
    ".builder\\\n",
    ".master(\"local\")\\\n",
    ".appName(\"holi\")\\\n",
    ".config(\"spark.some.config.option\", \"some-value\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## 1 · Crear Dataframes\n",
    "A **Dataset** is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). The Dataset API is available in Scala and Java.  \n",
    "\n",
    "A **DataFrame** is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.  \n",
    "They can be constructed from a wide array of sources such as,\n",
    "* Structured data files\n",
    "* Tables in Hive\n",
    "* External databases\n",
    "* Existing RDDs\n",
    "  \n",
    "DataFrame is represented by a **Dataset of Rows**. In Scala it's a type alias of Dataset \\[ Row \\]. In Java, Dataset < Row > *(Java)*\n",
    "\n",
    "### Structured Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distdata = sc.parallelize([(1,\"DataScientists\"),(2,\"Developers\"),(3,\"Scrums\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If **schema** is ***None*** SparkSQL infers the schema from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| _1|            _2|\n",
      "+---+--------------+\n",
      "|  1|DataScientists|\n",
      "|  2|    Developers|\n",
      "|  3|        Scrums|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DataFrameData = spark.createDataFrame(Distdata)\n",
    "DataFrameData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* If **schema** is a ***list of column names***, the __type of each column__ will be inferred from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|Importancia|         Curro|\n",
      "+-----------+--------------+\n",
      "|          1|DataScientists|\n",
      "|          2|    Developers|\n",
      "|          3|        Scrums|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DataFrameData = spark.createDataFrame(Distdata,[\"Importancia\",\"Curro\"])\n",
    "DataFrameData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* If **schema** is ***StructType***, which consists of ***a list of StructField*** with ***pyspark.sql.types objects***, those data types *must agree* with original data,else Python will throw an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|Importancia|         Curro|\n",
      "+-----------+--------------+\n",
      "|          1|DataScientists|\n",
      "|          2|    Developers|\n",
      "|          3|        Scrums|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"Importancia\", IntegerType(), True),\n",
    "    StructField(\"Curro\", StringType(), True)])\n",
    "DataFrameData = spark.createDataFrame(Distdata,schema)\n",
    "DataFrameData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Comma Separated Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clientes = spark.read.load(\"../data/clientes.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"false\")\n",
    "clientes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaClientes = (StructType([\n",
    "    StructField(\"DNI\", IntegerType(), True),\n",
    "    StructField(\"Nombre\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientsRDD = clientes.rdd\n",
    "type(clientsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|     DNI|              Nombre|             Address|\n",
      "+--------+--------------------+--------------------+\n",
      "|80000000|Antonio Lopez Ram...|Calle Cantalapied...|\n",
      "|70000000|Francisco Arias S...|Avenida de Americ...|\n",
      "|50000000|Norberto Marias Q...|     Calle Uganda 88|\n",
      "|10000000|Julio Cortazar Ca...|   Calle Bruselas 14|\n",
      "|20000000| Arturo Belano Yañez|Travesia de Calvo...|\n",
      "+--------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clientesNew = spark.createDataFrame(clientsRDD,schemaClientes)\n",
    "clientesNew.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Hadoop formats\n",
    "#### parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(registration_dttm=datetime.datetime(2016, 2, 3, 8, 55, 29), id=1, first_name='Amanda', last_name='Jordan', email='ajordan0@com.com', gender='Female', ip_address='1.197.201.2', cc='6759521864920116', country='Indonesia', birthdate='3/8/1971', salary=49756.53, title='Internal Auditor', comments='1E+02')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usuarios = spark.read.parquet(\"../data/userdata1.parquet\")\n",
    "usuarios.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(registration_dttm=datetime.datetime(2016, 2, 3, 8, 55, 29), id=1, first_name='Amanda', last_name='Jordan', email='ajordan0@com.com', gender='Female', ip_address='1.197.201.2', cc='6759521864920116', country='Indonesia', birthdate='3/8/1971', salary=49756.53, title='Internal Auditor', comments='1E+02')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usuarios2 = spark.read.format(\"parquet\").load(\"../data/userdata1.parquet\")\n",
    "usuarios2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- registration_dttm: timestamp (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- cc: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- birthdate: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usuarios.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "#### Avro\n",
    "In order to read this data format files, the appropiate .jar has to be previously loaded in the SparkContext. (See first command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+\n",
      "|  username|               tweet| timestamp|\n",
      "+----------+--------------------+----------+\n",
      "|    miguno|Rock: Nerf paper,...|1366150681|\n",
      "|BlizzardCS|Works as intended...|1366154481|\n",
      "+----------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter = spark.read.format(\"com.databricks.spark.avro\").load(\"../data/twitter.avro\")\n",
    "twitter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+\n",
      "|  username|               tweet| timestamp|\n",
      "+----------+--------------------+----------+\n",
      "|    miguno|Rock: Nerf paper,...|1366150681|\n",
      "|BlizzardCS|Works as intended...|1366154481|\n",
      "+----------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter2 = spark.read.load(\"../data/twitter.avro\",\n",
    "                     format=\"com.databricks.spark.avro\")\n",
    "twitter2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- username: string (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### pandas Import/Export\n",
    "\n",
    "#### Ensure PyArrow installed\n",
    "If you install PySpark using pip, then PyArrow can be brought in as an extra dependency of the SQL module with the command pip install pyspark[sql]. Otherwise, you must ensure that PyArrow is installed and available on all cluster nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.770794</td>\n",
       "      <td>0.468203</td>\n",
       "      <td>0.342891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.053756</td>\n",
       "      <td>0.127453</td>\n",
       "      <td>0.989758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.539427</td>\n",
       "      <td>0.278439</td>\n",
       "      <td>0.496780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.832618</td>\n",
       "      <td>0.866899</td>\n",
       "      <td>0.239864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.589955</td>\n",
       "      <td>0.530505</td>\n",
       "      <td>0.564471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.068579</td>\n",
       "      <td>0.477961</td>\n",
       "      <td>0.505941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.522635</td>\n",
       "      <td>0.281982</td>\n",
       "      <td>0.745112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.263354</td>\n",
       "      <td>0.675405</td>\n",
       "      <td>0.167618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.934501</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>0.545483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.077042</td>\n",
       "      <td>0.002963</td>\n",
       "      <td>0.094425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.482263</td>\n",
       "      <td>0.314019</td>\n",
       "      <td>0.585494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.110127</td>\n",
       "      <td>0.819074</td>\n",
       "      <td>0.020680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.659898</td>\n",
       "      <td>0.422138</td>\n",
       "      <td>0.431825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.823663</td>\n",
       "      <td>0.922017</td>\n",
       "      <td>0.952876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.594429</td>\n",
       "      <td>0.236816</td>\n",
       "      <td>0.928452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.250741</td>\n",
       "      <td>0.315057</td>\n",
       "      <td>0.087600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.673148</td>\n",
       "      <td>0.836862</td>\n",
       "      <td>0.311732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.323619</td>\n",
       "      <td>0.126083</td>\n",
       "      <td>0.593182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.572869</td>\n",
       "      <td>0.782080</td>\n",
       "      <td>0.044013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.232228</td>\n",
       "      <td>0.419137</td>\n",
       "      <td>0.706750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.645820</td>\n",
       "      <td>0.584320</td>\n",
       "      <td>0.524884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.096157</td>\n",
       "      <td>0.016875</td>\n",
       "      <td>0.565195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.980476</td>\n",
       "      <td>0.720327</td>\n",
       "      <td>0.418324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.654943</td>\n",
       "      <td>0.041669</td>\n",
       "      <td>0.137044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.194423</td>\n",
       "      <td>0.277260</td>\n",
       "      <td>0.180103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.072240</td>\n",
       "      <td>0.161823</td>\n",
       "      <td>0.559673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.787023</td>\n",
       "      <td>0.805124</td>\n",
       "      <td>0.461457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.246566</td>\n",
       "      <td>0.554753</td>\n",
       "      <td>0.112128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.694922</td>\n",
       "      <td>0.468174</td>\n",
       "      <td>0.786468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.640332</td>\n",
       "      <td>0.364350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.478542</td>\n",
       "      <td>0.999857</td>\n",
       "      <td>0.840165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.277038</td>\n",
       "      <td>0.920399</td>\n",
       "      <td>0.249623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.379123</td>\n",
       "      <td>0.809455</td>\n",
       "      <td>0.897390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.668960</td>\n",
       "      <td>0.413807</td>\n",
       "      <td>0.669783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.739754</td>\n",
       "      <td>0.425674</td>\n",
       "      <td>0.903481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.459641</td>\n",
       "      <td>0.181115</td>\n",
       "      <td>0.313950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.520176</td>\n",
       "      <td>0.258209</td>\n",
       "      <td>0.930511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.843919</td>\n",
       "      <td>0.892039</td>\n",
       "      <td>0.043264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.994054</td>\n",
       "      <td>0.165573</td>\n",
       "      <td>0.790476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.212906</td>\n",
       "      <td>0.378635</td>\n",
       "      <td>0.230877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.436010</td>\n",
       "      <td>0.997570</td>\n",
       "      <td>0.629424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.699700</td>\n",
       "      <td>0.061324</td>\n",
       "      <td>0.965481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.267743</td>\n",
       "      <td>0.676945</td>\n",
       "      <td>0.332093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.672862</td>\n",
       "      <td>0.421364</td>\n",
       "      <td>0.190241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.159043</td>\n",
       "      <td>0.229215</td>\n",
       "      <td>0.229358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.236740</td>\n",
       "      <td>0.541160</td>\n",
       "      <td>0.127764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.728681</td>\n",
       "      <td>0.152862</td>\n",
       "      <td>0.773401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.126271</td>\n",
       "      <td>0.524411</td>\n",
       "      <td>0.378348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.885895</td>\n",
       "      <td>0.418236</td>\n",
       "      <td>0.427574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.030159</td>\n",
       "      <td>0.626983</td>\n",
       "      <td>0.786127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.918231</td>\n",
       "      <td>0.729441</td>\n",
       "      <td>0.186012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.755942</td>\n",
       "      <td>0.341111</td>\n",
       "      <td>0.011140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.401948</td>\n",
       "      <td>0.486484</td>\n",
       "      <td>0.785418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.685504</td>\n",
       "      <td>0.369246</td>\n",
       "      <td>0.979288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.543720</td>\n",
       "      <td>0.170875</td>\n",
       "      <td>0.221625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.767905</td>\n",
       "      <td>0.697742</td>\n",
       "      <td>0.164879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.270644</td>\n",
       "      <td>0.789240</td>\n",
       "      <td>0.092090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.188504</td>\n",
       "      <td>0.703748</td>\n",
       "      <td>0.697513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.652389</td>\n",
       "      <td>0.086713</td>\n",
       "      <td>0.384494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.296599</td>\n",
       "      <td>0.163510</td>\n",
       "      <td>0.785026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2\n",
       "0   0.770794  0.468203  0.342891\n",
       "1   0.053756  0.127453  0.989758\n",
       "2   0.539427  0.278439  0.496780\n",
       "3   0.832618  0.866899  0.239864\n",
       "4   0.589955  0.530505  0.564471\n",
       "5   0.068579  0.477961  0.505941\n",
       "6   0.522635  0.281982  0.745112\n",
       "7   0.263354  0.675405  0.167618\n",
       "8   0.934501  0.097933  0.545483\n",
       "9   0.077042  0.002963  0.094425\n",
       "10  0.482263  0.314019  0.585494\n",
       "11  0.110127  0.819074  0.020680\n",
       "12  0.659898  0.422138  0.431825\n",
       "13  0.823663  0.922017  0.952876\n",
       "14  0.594429  0.236816  0.928452\n",
       "15  0.250741  0.315057  0.087600\n",
       "16  0.673148  0.836862  0.311732\n",
       "17  0.323619  0.126083  0.593182\n",
       "18  0.572869  0.782080  0.044013\n",
       "19  0.232228  0.419137  0.706750\n",
       "20  0.645820  0.584320  0.524884\n",
       "21  0.096157  0.016875  0.565195\n",
       "22  0.980476  0.720327  0.418324\n",
       "23  0.654943  0.041669  0.137044\n",
       "24  0.194423  0.277260  0.180103\n",
       "25  0.072240  0.161823  0.559673\n",
       "26  0.787023  0.805124  0.461457\n",
       "27  0.246566  0.554753  0.112128\n",
       "28  0.694922  0.468174  0.786468\n",
       "29  0.002976  0.640332  0.364350\n",
       "..       ...       ...       ...\n",
       "70  0.478542  0.999857  0.840165\n",
       "71  0.277038  0.920399  0.249623\n",
       "72  0.379123  0.809455  0.897390\n",
       "73  0.668960  0.413807  0.669783\n",
       "74  0.739754  0.425674  0.903481\n",
       "75  0.459641  0.181115  0.313950\n",
       "76  0.520176  0.258209  0.930511\n",
       "77  0.843919  0.892039  0.043264\n",
       "78  0.994054  0.165573  0.790476\n",
       "79  0.212906  0.378635  0.230877\n",
       "80  0.436010  0.997570  0.629424\n",
       "81  0.699700  0.061324  0.965481\n",
       "82  0.267743  0.676945  0.332093\n",
       "83  0.672862  0.421364  0.190241\n",
       "84  0.159043  0.229215  0.229358\n",
       "85  0.236740  0.541160  0.127764\n",
       "86  0.728681  0.152862  0.773401\n",
       "87  0.126271  0.524411  0.378348\n",
       "88  0.885895  0.418236  0.427574\n",
       "89  0.030159  0.626983  0.786127\n",
       "90  0.918231  0.729441  0.186012\n",
       "91  0.755942  0.341111  0.011140\n",
       "92  0.401948  0.486484  0.785418\n",
       "93  0.685504  0.369246  0.979288\n",
       "94  0.543720  0.170875  0.221625\n",
       "95  0.767905  0.697742  0.164879\n",
       "96  0.270644  0.789240  0.092090\n",
       "97  0.188504  0.703748  0.697513\n",
       "98  0.652389  0.086713  0.384494\n",
       "99  0.296599  0.163510  0.785026\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Capacitar la transferencia de datos columnares basados en Arrow\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "    \n",
    "# Generar un DataFrame de pandas\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Crear un DataFrame de Spark a partir de Pandas a través de Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Convertir el DataFrame de Spark de vuelta a Pandas a través de Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()\n",
    "\n",
    "result_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## 2 · DataFrames Operations\n",
    "\n",
    "We can access data in a column by attribute **(df.age)** and by index **(df['age'])**. Better the last way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"null\",\"Michael\"),(30,\"Andy\"),(19,\"Justin\"),(19,\"Alejandra\"),(19,\"Justin\")])\n",
    "df = spark.createDataFrame(rdd,[\"age\",\"name\"])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "|  19|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|     name|(age + 1)|\n",
      "+---------+---------+\n",
      "|  Michael|     null|\n",
      "|     Andy|     31.0|\n",
      "|   Justin|     20.0|\n",
      "|Alejandra|     20.0|\n",
      "|   Justin|     20.0|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['name'], df['age'] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "+----+---------+\n",
      "| age|     name|\n",
      "+----+---------+\n",
      "|  30|     Andy|\n",
      "|  19|Alejandra|\n",
      "|  19|   Justin|\n",
      "|null|  Michael|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.distinct().count())\n",
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  30|    1|\n",
      "|  19|    3|\n",
      "|null|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|     name|(age + 10)|\n",
      "+---------+----------+\n",
      "|  Michael|      null|\n",
      "|     Andy|      40.0|\n",
      "|   Justin|      29.0|\n",
      "|Alejandra|      29.0|\n",
      "|   Justin|      29.0|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"name\"], (df.age + 10)).show()\n",
    "# La función DataFrame.select(*cols)\n",
    "# *cols = lista de nombres de columna o expresiones (Columna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "There is a library plenty of **functions** designed for DataFrames formats, arithmetics... , called **pyspark.sql.functions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|media|\n",
      "+-----+\n",
      "|21.75|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as funciones\n",
    "df.select(funciones.avg(\"age\")\n",
    "          .alias(\"media\")\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|     name|new_age|\n",
      "+---------+-------+\n",
      "|  Michael|   null|\n",
      "|     Andy|  30.74|\n",
      "|   Justin|  19.74|\n",
      "|Alejandra|  19.74|\n",
      "|   Justin|  19.74|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select(df[\"name\"], (df.age + 0.74).alias(\"new_age\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|approximate_age|\n",
      "+---------------+\n",
      "|           null|\n",
      "|             31|\n",
      "|             20|\n",
      "|             20|\n",
      "|             20|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as funciones\n",
    "df2.select(funciones.ceil(\"new_age\").alias(\"approximate_age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## 3 · Running SQL Queries Programmatically\n",
    "To run SQL queries in a DataFrame, we have to create a **temporal view** in the SparkSQL session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|     name|new_age|\n",
      "+---------+-------+\n",
      "|  Michael|   null|\n",
      "|     Andy|  30.74|\n",
      "|   Justin|  19.74|\n",
      "|Alejandra|  19.74|\n",
      "|   Justin|  19.74|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     name|\n",
      "+---------+\n",
      "|Alejandra|\n",
      "|   Justin|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql2DF = spark.sql(\"SELECT name FROM people WHERE new_age<20\")\n",
    "sql2DF.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Running SQL Queries In Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----------+---------+----------------+------+--------------+----------------+---------+---------+---------+----------------+--------+\n",
      "|  registration_dttm| id|first_name|last_name|           email|gender|    ip_address|              cc|  country|birthdate|   salary|           title|comments|\n",
      "+-------------------+---+----------+---------+----------------+------+--------------+----------------+---------+---------+---------+----------------+--------+\n",
      "|2016-02-03 08:55:29|  1|    Amanda|   Jordan|ajordan0@com.com|Female|   1.197.201.2|6759521864920116|Indonesia| 3/8/1971| 49756.53|Internal Auditor|   1E+02|\n",
      "|2016-02-03 18:04:03|  2|    Albert|  Freeman| afreeman1@is.gd|  Male|218.111.175.34|                |   Canada|1/16/1968|150280.17|   Accountant IV|        |\n",
      "+-------------------+---+----------+---------+----------------+------+--------------+----------------+---------+---------+---------+----------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SQLparquetQuery = spark.sql(\"SELECT * FROM parquet.`../data/userdata1.parquet`\")\n",
    "SQLparquetQuery.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## 4 · Global Temporary Views\n",
    "Temporary views in Spark SQL are session-scoped and will disappear if the session that creates it terminates. If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, you can create a global temporary view. **Global temporary view** is tied to a system preserved database **global_temp**, and we must use the qualified name to refer it, e.g. SELECT * FROM global_temp.view1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createGlobalTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "| age|     name|\n",
      "+----+---------+\n",
      "|null|  Michael|\n",
      "|  30|     Andy|\n",
      "|  19|   Justin|\n",
      "|  19|Alejandra|\n",
      "|  19|   Justin|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * TemporaryFROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "| age|     name|\n",
      "+----+---------+\n",
      "|null|  Michael|\n",
      "|  30|     Andy|\n",
      "|  19|   Justin|\n",
      "|  19|Alejandra|\n",
      "|  19|   Justin|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
